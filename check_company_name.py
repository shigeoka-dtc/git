import os
import re
import time
import json
import random
import logging
import argparse
import urllib.parse
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
import traceback

# âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«
CACHE_FILE = "bing_cache_v6_final_full.json"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def get_driver():
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1200,800")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)
    return driver

# âœ… ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚¹ã‚³ã‚¢è¨­å®š
DOMAIN_PRIORITY = [
    ".co.jp", ".go.jp", ".or.jp",
    "prtimes.jp", "news.yahoo.co.jp", "nikkei.com",
    "businessinsider.jp", "itmedia.co.jp",
    "impress.co.jp", "reuters.com", "asahi.com",
    "mainichi.jp", "yomiuri.co.jp",
    "sankei.com", "jiji.com", "nhk.or.jp"
]

LOW_QUALITY_DOMAINS = [
    "genspark.ai", "reflet-office.com", "note.com", "qiita.com", "zenn.dev",
    "office-tsuda.net", "advisors-freee.jp", "freee.co.jp",
    "houmukyoku.moj.go.jp", "bing.com/ck/a",
    "ai-con.lawyer", "shiodome.co.jp",
    "zeiri4.com", "bizocean.jp", "corporate.ai-con.lawyer", "kaonavi.jp"
]

def domain_score(url):
    url = url or ""
    for domain in LOW_QUALITY_DOMAINS:
        if domain in url:
            return -100
    for i, domain in enumerate(DOMAIN_PRIORITY):
        if domain in url:
            return len(DOMAIN_PRIORITY) - i
    return 0

# âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
def is_low_quality(snippet, url):
    low_keywords = [
        "å•†å·å¤‰æ›´ã¨ã¯", "ç¤¾åå¤‰æ›´ã¨ã¯", "ä¼šç¤¾åãŒå¤‰æ›´ã«ãªã‚‹å ´åˆã¯"
    ]
    snippet = snippet or ""
    url = url or ""
    for kw in low_keywords:
        if kw in snippet or kw in url:
            return True
    return False

# âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰éƒ¨åˆ†ä¸€è‡´
def company_in_text(company, text):
    keyword = company.replace("æ ªå¼ä¼šç¤¾", "").strip()
    return re.search(re.escape(keyword), text, re.IGNORECASE) is not None

# âœ… ã‚¹ã‚³ã‚¢è¨ˆç®—
def result_score(company, title, snippet, url):
    score = domain_score(url)
    if company_in_text(company, title + snippet):
        score += 5
    return score

# âœ… Bingæ¤œç´¢
def search_bing(driver, company):
    query = f"{company} ç¤¾åå¤‰æ›´ OR å•†å·å¤‰æ›´ OR æ–°ç¤¾å"
    url = f"https://www.bing.com/search?q={urllib.parse.quote(query)}"
    driver.get(url)
    time.sleep(random.uniform(1.5, 4.5))
    results = []
    for elem in driver.find_elements(By.CSS_SELECTOR, "li.b_algo")[:10]:
        try:
            title = elem.find_element(By.TAG_NAME, "h2").text
            snippet = elem.find_element(By.CLASS_NAME, "b_caption").text
            link = elem.find_element(By.TAG_NAME, "a").get_attribute("href")
            results.append((title + "\n" + snippet, snippet, link))
        except Exception as e:
            logging.debug(f"æ¤œç´¢çµæœè§£æã‚¨ãƒ©ãƒ¼: {e}")
            continue
    return results

# ğŸš« é™¤å¤–ãƒ¯ãƒ¼ãƒ‰
EXCLUDE_NAME_PATTERNS = [
    r"æ­£å¼ã«ã¯", r"é€šç§°", r"å‘¼ã°ã‚Œ", r"ä¸€èˆ¬çš„ã«", r"ç•¥ç§°", r"é€šå", r"ä¼šç¤¾åã¨ã¯", r"ç¤¾åã¨ã¯"
]

# ğŸš« BAD_NAMES å¼·åŒ–
BAD_NAMES = [
    "å½“ãƒšãƒ¼ã‚¸ã‚’å‚è€ƒ", "ã“ã¡ã‚‰", "ä¸æ˜", "å‚è€ƒ", "ç¤¾åã¯", "ã¨ã„ã„ã¾ã™", "æ­£å¼ã«ã¯", "å•†å·", "ç¤¾åå¤‰æ›´ã¨ã¯"
]

# âœ… extract_info æ”¹è‰¯ç‰ˆ
def extract_info(text, old_name):
    if any(re.search(pat, text) for pat in EXCLUDE_NAME_PATTERNS):
        return None, None, None

    name_match = re.search(
        r'ç¤¾å(?:ã‚’)?ã€Œ?([^\sã€Œã€]{2,})ã€?ã«å¤‰æ›´|'
        r'æ–°ç¤¾åã¯ã€Œ?([^\sã€Œã€]{2,})ã€?|'
        r'ã€Œ?([^\sã€Œã€]{2,})æ ªå¼ä¼šç¤¾ã€?ã«å¤‰æ›´|'
        r'([^\sã€Œã€]{2,})æ ªå¼ä¼šç¤¾ã«å¤‰æ›´', 
        text
    )

    date_match = re.search(r"(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥|\d{4}å¹´\d{1,2}æœˆ|\d{4}å¹´)", text)
    reason_match = re.search(r"(ç†ç”±ã¯[^ã€‚]{3,15})ã€‚", text)

    new_name = None
    if name_match:
        for g in name_match.groups():
            if g and g not in BAD_NAMES and not g.startswith("ã¯"):
                new_name = g
                break
    if not new_name:
        return None, None, None

    if old_name.replace("æ ªå¼ä¼šç¤¾", "").strip() in new_name:
        return None, None, None

    date = date_match.group(1) if date_match else "å¤‰æ›´æ—¥ä¸æ˜"
    reason = reason_match.group(1).replace("ç†ç”±ã¯", "") if reason_match else "ä¸æ˜"

    return new_name, date, reason

# âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ“ä½œ
def load_cache():
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_cache(cache):
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

# âœ… ä¼šç¤¾1ç¤¾ãšã¤å‡¦ç†
def analyze_company(company):
    cache = load_cache()
    key = company.strip().lower()

    if key in cache:
        logging.info(f"ã€RESUMEã€‘ã‚¹ã‚­ãƒƒãƒ—: {company}")
        result = cache[key]
        if result[4] == "ã‚¹ã‚­ãƒƒãƒ—":
            result[4] = "å¤‰æ›´ãªã—"
        return result

    driver = None
    try:
        logging.info(f"æ¤œç´¢é–‹å§‹: {company}")
        driver = get_driver()
        results = search_bing(driver, company)

        results_sorted = sorted(
            [r for r in results if not is_low_quality(r[1], r[2])],
            key=lambda x: result_score(company, x[0], x[1], x[2]),
            reverse=True
        )

        for full_text, snippet, url in results_sorted:
            new_name, date, reason = extract_info(full_text, company)
            if new_name:
                result = [company, new_name, date, reason, "å¤‰æ›´ã‚ã‚Š", snippet or "ãªã—", url or ""]
                cache[key] = result
                save_cache(cache)
                return result

        if results_sorted:
            snippet = results_sorted[0][1] or "ãªã—"
            url = results_sorted[0][2] or ""
        else:
            snippet = "ãªã—"
            url = ""

        result = [company, "å¤‰æ›´ãªã—", "å¤‰æ›´æ—¥ä¸æ˜", "ä¸æ˜", "å¤‰æ›´ãªã—", snippet, url]
        cache[key] = result
        save_cache(cache)
        return result

    except Exception as e:
        logging.error(f"ã‚¨ãƒ©ãƒ¼: {company} - {e}")
        logging.error(traceback.format_exc())
        return [company, "ã‚¨ãƒ©ãƒ¼", "ä¸æ˜", "ä¸æ˜", "å‡¦ç†å¤±æ•—", str(e), ""]
    finally:
        if driver:
            driver.quit()

# âœ… ä¸¦åˆ—å‡¦ç†
def process_all(companies):
    max_workers = 6
    logging.info(f"ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {max_workers}")
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        results = list(tqdm(executor.map(lambda c: analyze_company(c), companies), total=len(companies)))
    return results

# âœ… ãƒ¡ã‚¤ãƒ³
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("input", help="ä¼šç¤¾åCSVãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("output", help="å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«")
    args = parser.parse_args()

    df = pd.read_csv(args.input)
    companies = df["ä¼šç¤¾å"].dropna().tolist()
    logging.info(f"å¯¾è±¡ç¤¾æ•°: {len(companies)}ç¤¾")

    results_dict = {}
    all_results = []

    for result in process_all(companies):
        key = result[0].strip().lower()
        results_dict[key] = result
        all_results.append(result)

    df_out_rows = []
    for company in companies:
        key = company.strip().lower()
        result = results_dict.get(key)
        if result is None:
            result = analyze_company(company)
        df_out_rows.append(result)

    df_out = pd.DataFrame(df_out_rows, columns=[
        "ä¼šç¤¾å", "æ–°ç¤¾å", "å¤‰æ›´æ—¥", "å¤‰æ›´ç†ç”±", "å¤‰æ›´çŠ¶æ³", "æ¤œå‡ºæ–‡", "URL"
    ])
    df_out.to_csv(args.output, index=False, encoding="utf-8-sig")
    logging.info(f"å‡ºåŠ›å®Œäº†: {args.output}")

if __name__ == "__main__":
    main()
